{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takaaki-yayoi/langchain_tutorial/blob/main/langchain_tutorial_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTDrBWhAwrrA"
      },
      "source": [
        "# ğŸ¦œğŸ”— LangChain å…¥é–€ã‚¬ã‚¤ãƒ‰\n",
        "\n",
        "## ğŸ“š ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã«ã¤ã„ã¦\n",
        "\n",
        "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€LangChainã®åŸºæœ¬æ¦‚å¿µã‹ã‚‰å®Ÿè·µçš„ãªä½¿ã„æ–¹ã¾ã§ã€æ®µéšçš„ã«å­¦ç¿’ã—ã¦ã„ãã¾ã™ã€‚\n",
        "\n",
        "### å­¦ç¿’å†…å®¹\n",
        "1. **ç’°å¢ƒæ§‹ç¯‰** - å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "2. **åŸºæœ¬æ¦‚å¿µ** - LLMã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã€ãƒã‚§ãƒ¼ãƒ³\n",
        "3. **å®Ÿè·µæ¼”ç¿’** - ç°¡å˜ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®æ§‹ç¯‰\n",
        "4. **å¿œç”¨ä¾‹** - RAGï¼ˆRetrieval-Augmented Generationï¼‰ã®å®Ÿè£…\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkkKuR8BwrrC"
      },
      "source": [
        "## ğŸ“¦ Step 1: ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
        "\n",
        "ã¾ãšã¯å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã—ã‚‡ã†ã€‚Google Colabã§ã¯ã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ã ã‘ã§æº–å‚™å®Œäº†ã§ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOKy7GYkwrrD",
        "outputId": "b8327f66-e3ef-411d-c919-38376e611446"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "âœ… ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†ï¼\n"
          ]
        }
      ],
      "source": [
        "# åŸºæœ¬çš„ãªLangChainãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "!pip install -q langchain langchain-community langchain-openai\n",
        "\n",
        "# è¿½åŠ ã®ä¾¿åˆ©ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸\n",
        "!pip install -q chromadb tiktoken python-dotenv\n",
        "\n",
        "print(\"âœ… ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†ï¼\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKHGYqXPwrrD"
      },
      "source": [
        "### ğŸ”‘ API ã‚­ãƒ¼ã®è¨­å®š\n",
        "\n",
        "LangChainã§LLMã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™ã€‚ã“ã“ã§ã¯OpenAIã‚’ä¾‹ã«è¨­å®šæ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚\n",
        "\n",
        "**é‡è¦**: APIã‚­ãƒ¼ã¯çµ¶å¯¾ã«å…¬é–‹ã—ãªã„ã§ãã ã•ã„ï¼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLzuOQuzwrrD",
        "outputId": "dc8e5cd2-133e-44fe-f9a4-e593e0a84553"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¾ã—ãŸ\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Google Colabã®ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆæ©Ÿèƒ½ã‚’ä½¿ã†ã‹ã€ç›´æ¥å…¥åŠ›\n",
        "# æ–¹æ³•1: ç›´æ¥å…¥åŠ›ï¼ˆã‚»ã‚­ãƒ¥ã‚¢ãªå…¥åŠ›ï¼‰\n",
        "#os.environ[\"openai_api_key\"] = getpass(\"OpenAI APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: \")\n",
        "\n",
        "# æ–¹æ³•2: Google Colabã®ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆæ©Ÿèƒ½ã‚’ä½¿ã†å ´åˆ\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai_api_key')\n",
        "\n",
        "print(\"âœ… APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¾ã—ãŸ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veR9XFSdwrrE"
      },
      "source": [
        "---\n",
        "\n",
        "## ğŸ¯ Step 2: LangChainã®åŸºæœ¬æ¦‚å¿µ\n",
        "\n",
        "### 2.1 LLMï¼ˆLarge Language Modelï¼‰ã®åŸºæœ¬\n",
        "\n",
        "LangChainã®ä¸­å¿ƒã¨ãªã‚‹ã®ã¯LLMã§ã™ã€‚ã¾ãšã¯æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªä½¿ã„æ–¹ã‹ã‚‰å§‹ã‚ã¾ã—ã‚‡ã†ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SK-SlT0wrrE",
        "outputId": "2d3ed61d-896a-4863-85c9-fba81c1a4f46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¤– AIã®å›ç­”:\n",
            "LangChainã¯ã€è¨€èªç¿»è¨³æŠ€è¡“ã‚’æ´»ç”¨ã—ã¦ã€ç•°ãªã‚‹è¨€èªé–“ã§ã®ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ”¯æ´ã™ã‚‹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‚„ã‚µãƒ¼ãƒ“ã‚¹ã®ã“ã¨ã‚’æŒ‡ã—ã¾ã™ã€‚\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# LLMã®åˆæœŸåŒ–\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0.7,  # å‰µé€ æ€§ã®ãƒ¬ãƒ™ãƒ«ï¼ˆ0-1ï¼‰\n",
        ")\n",
        "\n",
        "# ã‚·ãƒ³ãƒ—ãƒ«ãªä½¿ç”¨ä¾‹\n",
        "response = llm.invoke(\"LangChainã¨ã¯ä½•ã§ã™ã‹ï¼Ÿæ—¥æœ¬èªã§ç°¡æ½”ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\")\n",
        "print(\"ğŸ¤– AIã®å›ç­”:\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWpAslvswrrE"
      },
      "source": [
        "### 2.2 ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ\n",
        "\n",
        "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ã†ã¨ã€å‹•çš„ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã§ãã¾ã™ã€‚ã“ã‚Œã¯å®Ÿéš›ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹ç™ºã§éå¸¸ã«é‡è¦ã§ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6rCr87cwrrE",
        "outputId": "af1ba75f-bbf4-44e9-ce79-46b66c2ab0d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“ ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã•ã‚ŒãŸå›ç­”:\n",
            "ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæŒ‡å‘ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ï¼ˆObject-Oriented Programmingã€OOPï¼‰ã¯ã€ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®è€ƒãˆæ–¹ã®ä¸€ã¤ã§ã€ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨å‘¼ã°ã‚Œã‚‹è¦ç´ ã«åˆ†å‰²ã—ã¦è¨­è¨ˆã™ã‚‹æ–¹æ³•ã§ã™ã€‚\n",
            "\n",
            "ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¯ã€ãã‚Œãã‚Œç‰¹å®šã®å½¹å‰²ã‚„æ©Ÿèƒ½ã‚’æŒã£ã¦ãŠã‚Šã€ä»–ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨å”åŠ›ã—ã¦å‡¦ç†ã‚’è¡Œã„ã¾ã™ã€‚ä¾‹ãˆã°ã€è‡ªå‹•è»Šã¨ã„ã†ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒã‚ã‚‹å ´åˆã€ãã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«ã¯ã€Œèµ°ã‚‹ã€ã¨ã„ã†æ©Ÿèƒ½ã‚„ã€Œè‰²ã€ã¨ã„ã†ç‰¹æ€§ãŒã‚ã‚‹ã¨è€ƒãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
            "\n",
            "ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæŒ‡å‘ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã§ã¯ã€ã“ã®ã‚ˆã†ãªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆåŒå£«ã®ç›¸äº’ä½œç”¨ã‚’é€šã˜ã¦ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚å…·ä½“çš„ã«ã¯ã€ã‚¯ãƒ©ã‚¹ã¨ã„ã†è¨­è¨ˆå›³ã‚’ä½¿ã£ã¦ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆã—ã€ãã‚Œã‚‰ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆåŒå£«ãŒãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚„ã‚Šå–ã‚Šã—ã¦å‡¦ç†ã‚’è¡Œã„ã¾ã™ã€‚\n",
            "\n",
            "ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæŒ‡å‘ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®åˆ©ç‚¹ã¨ã—ã¦ã€ã‚³ãƒ¼ãƒ‰ã®å†åˆ©ç”¨æ€§ã‚„ä¿å®ˆæ€§ãŒé«˜ããªã‚‹ã“ã¨ã€ã‚·ã‚¹ãƒ†ãƒ ã®æŸ”è»Ÿæ€§ã‚„æ‹¡å¼µæ€§ãŒå‘ä¸Šã™ã‚‹ã“ã¨ã€ã‚³ãƒ¼ãƒ‰ã®ç†è§£ãŒã—ã‚„ã™ããªã‚‹ã“ã¨ãªã©ãŒæŒ™ã’ã‚‰ã‚Œã¾ã™ã€‚\n",
            "\n",
            "åˆå¿ƒè€…ãŒã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæŒ‡å‘ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚’å­¦ã¶éš›ã«ã¯ã€ã¾ãšåŸºæœ¬çš„ãªæ¦‚å¿µã‚„ç”¨èªã‚’ç†è§£ã—ã€å®Ÿéš›ã«ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’æ›¸ããªãŒã‚‰ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæŒ‡å‘ã®è€ƒãˆæ–¹ã‚’ä½“é¨“ã™ã‚‹ã“ã¨ãŒå¤§åˆ‡ã§ã™ã€‚\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ä½œæˆ\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"ã‚ãªãŸã¯{role}ã®å°‚é–€å®¶ã§ã™ã€‚\"),\n",
        "    (\"human\", \"{topic}ã«ã¤ã„ã¦ã€åˆå¿ƒè€…ã«ã‚‚ã‚ã‹ã‚‹ã‚ˆã†ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\")\n",
        "])\n",
        "\n",
        "# ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ä½¿ç”¨\n",
        "prompt = template.format_messages(\n",
        "    role=\"ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°æ•™è‚²\",\n",
        "    topic=\"ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæŒ‡å‘ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°\"\n",
        ")\n",
        "\n",
        "response = llm.invoke(prompt)\n",
        "print(\"ğŸ“ ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã•ã‚ŒãŸå›ç­”:\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1HeuI50wrrE"
      },
      "source": [
        "### 2.3 ãƒã‚§ãƒ¼ãƒ³ï¼ˆChainï¼‰- LangChainã®æ ¸å¿ƒ\n",
        "\n",
        "ãƒã‚§ãƒ¼ãƒ³ã¯è¤‡æ•°ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’é€£çµã—ã¦ã€ã‚ˆã‚Šè¤‡é›‘ãªå‡¦ç†ã‚’å®Ÿç¾ã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJYO5bc_wrrE",
        "outputId": "7f6cea61-a39e-4063-e7c1-6beb2b7f3361"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4102117786.py:10: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  summarize_chain = LLMChain(\n",
            "/tmp/ipython-input-4102117786.py:47: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  result = overall_chain({\"text\": sample_text})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "ğŸ“ è¦ç´„çµæœ:\n",
            "LangChainã¯LLMã‚’åˆ©ç”¨ã—ãŸã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹ç™ºã‚’ç°¡ç´ åŒ–ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€‚\n",
            "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†ã‚„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆãªã©ã€æ§˜ã€…ãªæ©Ÿèƒ½ã‚’æä¾›ã€‚\n",
            "é–‹ç™ºè€…ã¯ã“ã‚Œã‚‰ã‚’çµ„ã¿åˆã‚ã›ã¦è¤‡é›‘ãªAIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’åŠ¹ç‡çš„ã«æ§‹ç¯‰ã§ãã‚‹ã€‚\n",
            "\n",
            "ğŸŒ è‹±èªç¿»è¨³:\n",
            "LangChain is a framework that simplifies application development using LLM. It provides various functions such as prompt management and agent creation. Developers can efficiently build complex AI applications by combining these functions.\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# ã‚¹ãƒ†ãƒƒãƒ—1: è¦ç´„ç”¨ã®ãƒã‚§ãƒ¼ãƒ³\n",
        "summarize_template = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=\"ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’3è¡Œã§è¦ç´„ã—ã¦ãã ã•ã„:\\n\\n{text}\"\n",
        ")\n",
        "\n",
        "summarize_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=summarize_template,\n",
        "    output_key=\"summary\"\n",
        ")\n",
        "\n",
        "# ã‚¹ãƒ†ãƒƒãƒ—2: ç¿»è¨³ç”¨ã®ãƒã‚§ãƒ¼ãƒ³\n",
        "translate_template = PromptTemplate(\n",
        "    input_variables=[\"summary\"],\n",
        "    template=\"ä»¥ä¸‹ã®æ—¥æœ¬èªã‚’è‹±èªã«ç¿»è¨³ã—ã¦ãã ã•ã„:\\n\\n{summary}\"\n",
        ")\n",
        "\n",
        "translate_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=translate_template,\n",
        "    output_key=\"translation\"\n",
        ")\n",
        "\n",
        "# ãƒã‚§ãƒ¼ãƒ³ã®é€£çµ\n",
        "from langchain.chains import SequentialChain\n",
        "\n",
        "overall_chain = SequentialChain(\n",
        "    chains=[summarize_chain, translate_chain],\n",
        "    input_variables=[\"text\"],\n",
        "    output_variables=[\"summary\", \"translation\"],\n",
        "    verbose=True  # å‡¦ç†éç¨‹ã‚’è¡¨ç¤º\n",
        ")\n",
        "\n",
        "# å®Ÿè¡Œä¾‹\n",
        "sample_text = \"\"\"\n",
        "LangChainã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã‚’æ´»ç”¨ã—ãŸã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹ç™ºã‚’\n",
        "ç°¡ç´ åŒ–ã™ã‚‹ãŸã‚ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ç®¡ç†ã€ãƒã‚§ãƒ¼ãƒ³ã®æ§‹ç¯‰ã€\n",
        "ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ä½œæˆã€ãƒ¡ãƒ¢ãƒªã®å®Ÿè£…ãªã©ã€LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«å¿…è¦ãª\n",
        "æ§˜ã€…ãªæ©Ÿèƒ½ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚é–‹ç™ºè€…ã¯ã€ã“ã‚Œã‚‰ã®æ©Ÿèƒ½ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€\n",
        "è¤‡é›‘ãªAIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’åŠ¹ç‡çš„ã«æ§‹ç¯‰ã§ãã¾ã™ã€‚\n",
        "\"\"\"\n",
        "\n",
        "result = overall_chain({\"text\": sample_text})\n",
        "print(\"\\nğŸ“ è¦ç´„çµæœ:\")\n",
        "print(result[\"summary\"])\n",
        "print(\"\\nğŸŒ è‹±èªç¿»è¨³:\")\n",
        "print(result[\"translation\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkE22XvIwrrE"
      },
      "source": [
        "---\n",
        "\n",
        "## ğŸš€ Step 3: å®Ÿè·µçš„ãªå¿œç”¨ä¾‹\n",
        "\n",
        "### 3.1 ç°¡å˜ãªQ&Aã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOvY1YYbwrrE",
        "outputId": "206131a9-d3af-4a99-ada5-0bd6ca721924"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2492183815.py:5: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory()\n",
            "/tmp/ipython-input-2492183815.py:6: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
            "  conversation = ConversationChain(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¬ ä¼šè©±å‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ\n",
            "='exit'ã¨å…¥åŠ›ã™ã‚‹ã¨çµ‚äº†ã—ã¾ã™ï¼‰\n",
            "\n",
            "\n",
            "ğŸ‘¤ You: ç§ã®åå‰ã¯å¤ªéƒã§ã™ã€‚ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚’å‹‰å¼·ã—ã¦ã„ã¾ã™ã€‚\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: ç§ã®åå‰ã¯å¤ªéƒã§ã™ã€‚ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚’å‹‰å¼·ã—ã¦ã„ã¾ã™ã€‚\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "ğŸ¤– AI: ã“ã‚“ã«ã¡ã¯ã€å¤ªéƒã•ã‚“ï¼ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚’å‹‰å¼·ã—ã¦ã„ã‚‹ã‚“ã§ã™ã­ã€‚ã©ã‚“ãªè¨€èªã‚’å‹‰å¼·ã—ã¦ã„ã¾ã™ã‹ï¼Ÿç§ã¯Pythonã‚„JavaScriptãªã©ã®è¨€èªã«è©³ã—ã„ã§ã™ã‚ˆã€‚ä½•ã‹è³ªå•ãŒã‚ã‚Œã°ãŠæ°—è»½ã«ã©ã†ãï¼\n",
            "\n",
            "ğŸ‘¤ You: ç§ã®åå‰ã‚’è¦šãˆã¦ã„ã¾ã™ã‹ï¼Ÿ\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: ç§ã®åå‰ã¯å¤ªéƒã§ã™ã€‚ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚’å‹‰å¼·ã—ã¦ã„ã¾ã™ã€‚\n",
            "AI: ã“ã‚“ã«ã¡ã¯ã€å¤ªéƒã•ã‚“ï¼ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚’å‹‰å¼·ã—ã¦ã„ã‚‹ã‚“ã§ã™ã­ã€‚ã©ã‚“ãªè¨€èªã‚’å‹‰å¼·ã—ã¦ã„ã¾ã™ã‹ï¼Ÿç§ã¯Pythonã‚„JavaScriptãªã©ã®è¨€èªã«è©³ã—ã„ã§ã™ã‚ˆã€‚ä½•ã‹è³ªå•ãŒã‚ã‚Œã°ãŠæ°—è»½ã«ã©ã†ãï¼\n",
            "Human: ç§ã®åå‰ã‚’è¦šãˆã¦ã„ã¾ã™ã‹ï¼Ÿ\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "ğŸ¤– AI: ã¯ã„ã€å¤ªéƒã•ã‚“ã§ã™ã­ï¼è¦šãˆã¦ã„ã¾ã™ã‚ˆã€‚ä»–ã«ã‚‚ä½•ã‹çŸ¥ã‚ŠãŸã„ã“ã¨ãŒã‚ã‚Œã°ã©ã†ããŠå°‹ã­ãã ã•ã„ã­ã€‚\n",
            "\n",
            "ğŸ‘¤ You: ç§ã¯ä½•ã‚’å‹‰å¼·ã—ã¦ã„ã‚‹ã¨è¨€ã„ã¾ã—ãŸã‹ï¼Ÿ\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: ç§ã®åå‰ã¯å¤ªéƒã§ã™ã€‚ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚’å‹‰å¼·ã—ã¦ã„ã¾ã™ã€‚\n",
            "AI: ã“ã‚“ã«ã¡ã¯ã€å¤ªéƒã•ã‚“ï¼ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚’å‹‰å¼·ã—ã¦ã„ã‚‹ã‚“ã§ã™ã­ã€‚ã©ã‚“ãªè¨€èªã‚’å‹‰å¼·ã—ã¦ã„ã¾ã™ã‹ï¼Ÿç§ã¯Pythonã‚„JavaScriptãªã©ã®è¨€èªã«è©³ã—ã„ã§ã™ã‚ˆã€‚ä½•ã‹è³ªå•ãŒã‚ã‚Œã°ãŠæ°—è»½ã«ã©ã†ãï¼\n",
            "Human: ç§ã®åå‰ã‚’è¦šãˆã¦ã„ã¾ã™ã‹ï¼Ÿ\n",
            "AI: ã¯ã„ã€å¤ªéƒã•ã‚“ã§ã™ã­ï¼è¦šãˆã¦ã„ã¾ã™ã‚ˆã€‚ä»–ã«ã‚‚ä½•ã‹çŸ¥ã‚ŠãŸã„ã“ã¨ãŒã‚ã‚Œã°ã©ã†ããŠå°‹ã­ãã ã•ã„ã­ã€‚\n",
            "Human: ç§ã¯ä½•ã‚’å‹‰å¼·ã—ã¦ã„ã‚‹ã¨è¨€ã„ã¾ã—ãŸã‹ï¼Ÿ\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "ğŸ¤– AI: ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚’å‹‰å¼·ã—ã¦ã„ã‚‹ã¨ãŠã£ã—ã‚ƒã„ã¾ã—ãŸã­ï¼ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã¯ã¨ã¦ã‚‚é¢ç™½ã„ã§ã™ã‚ˆã­ã€‚ä½•ã‹å…·ä½“çš„ãªã“ã¨ã‚’ä½œæˆã—ã¦ã„ã¾ã™ã‹ï¼Ÿã‚‚ã—ã”è³ªå•ãŒã‚ã‚Œã°ãŠç­”ãˆã—ã¾ã™ã®ã§ãŠçŸ¥ã‚‰ã›ãã ã•ã„ã­ã€‚\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "# ãƒ¡ãƒ¢ãƒªä»˜ãã®ä¼šè©±ãƒã‚§ãƒ¼ãƒ³ã‚’ä½œæˆ\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"ğŸ’¬ ä¼šè©±å‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ\")\n",
        "print(\"='exit'ã¨å…¥åŠ›ã™ã‚‹ã¨çµ‚äº†ã—ã¾ã™ï¼‰\\n\")\n",
        "\n",
        "# ä¼šè©±ã®ãƒ‡ãƒ¢\n",
        "questions = [\n",
        "    \"ç§ã®åå‰ã¯å¤ªéƒã§ã™ã€‚ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚’å‹‰å¼·ã—ã¦ã„ã¾ã™ã€‚\",\n",
        "    \"ç§ã®åå‰ã‚’è¦šãˆã¦ã„ã¾ã™ã‹ï¼Ÿ\",\n",
        "    \"ç§ã¯ä½•ã‚’å‹‰å¼·ã—ã¦ã„ã‚‹ã¨è¨€ã„ã¾ã—ãŸã‹ï¼Ÿ\"\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    print(f\"\\nğŸ‘¤ You: {question}\")\n",
        "    response = conversation.predict(input=question)\n",
        "    print(f\"ğŸ¤– AI: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTZLGj-xwrrF"
      },
      "source": [
        "### 3.2 ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåˆ†æï¼ˆç°¡æ˜“RAGï¼‰\n",
        "\n",
        "RAGï¼ˆRetrieval-Augmented Generationï¼‰ã¯ã€å¤–éƒ¨çŸ¥è­˜ã‚’æ´»ç”¨ã—ã¦ã‚ˆã‚Šæ­£ç¢ºãªå›ç­”ã‚’ç”Ÿæˆã™ã‚‹æŠ€è¡“ã§ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1oYzNH5wrrF",
        "outputId": "15a682d9-c030-445e-e607-6b39eb2a4091"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â“ è³ªå•: LangChainã‚’ä½œã£ãŸäººã¯èª°ã§ã™ã‹ï¼Ÿ\n",
            "\n",
            "ğŸ’¡ å›ç­”: LangChainã¯Harrison Chaseã«ã‚ˆã£ã¦ä½œæˆã•ã‚Œã¾ã—ãŸã€‚\n",
            "\n",
            "ğŸ“š å‚ç…§ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ:\n",
            "  - LangChainã¯2022å¹´10æœˆã«Harrison Chaseã«ã‚ˆã£ã¦ä½œæˆã•ã‚Œã¾ã—ãŸã€‚\n",
            "    ...\n",
            "  - LangChainã¯ã€Pythonç‰ˆã¨JavaScript/TypeScriptç‰ˆãŒæä¾›ã•ã‚Œã¦ãŠã‚Šã€...\n",
            "  - LangChainã®ä¸»ãªæ©Ÿèƒ½ã«ã¯ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã€ãƒã‚§ãƒ¼ãƒ³ã€\n",
            "    ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€ãƒ¡ãƒ¢ãƒªç®¡...\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# ã‚µãƒ³ãƒ—ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ\n",
        "documents = [\n",
        "    \"\"\"LangChainã¯2022å¹´10æœˆã«Harrison Chaseã«ã‚ˆã£ã¦ä½œæˆã•ã‚Œã¾ã—ãŸã€‚\n",
        "    ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¨ã—ã¦ã€GitHubã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚\"\"\",\n",
        "\n",
        "    \"\"\"LangChainã®ä¸»ãªæ©Ÿèƒ½ã«ã¯ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã€ãƒã‚§ãƒ¼ãƒ³ã€\n",
        "    ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€ãƒ¡ãƒ¢ãƒªç®¡ç†ãªã©ãŒã‚ã‚Šã¾ã™ã€‚\"\"\",\n",
        "\n",
        "    \"\"\"LangChainã¯ã€Pythonç‰ˆã¨JavaScript/TypeScriptç‰ˆãŒæä¾›ã•ã‚Œã¦ãŠã‚Šã€\n",
        "    æ§˜ã€…ãªLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼ˆOpenAIã€Anthropicã€Googleãªã©ï¼‰ã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚\"\"\"\n",
        "]\n",
        "\n",
        "# ãƒ†ã‚­ã‚¹ãƒˆã‚’å°ã•ãªãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²\n",
        "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
        "texts = text_splitter.create_documents(documents)\n",
        "\n",
        "# ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®ä½œæˆ\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = Chroma.from_documents(texts, embeddings)\n",
        "\n",
        "# è³ªå•å¿œç­”ãƒã‚§ãƒ¼ãƒ³ã®ä½œæˆ\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectorstore.as_retriever(),\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "# è³ªå•ã—ã¦ã¿ã‚‹\n",
        "question = \"LangChainã‚’ä½œã£ãŸäººã¯èª°ã§ã™ã‹ï¼Ÿ\"\n",
        "result = qa_chain({\"query\": question})\n",
        "\n",
        "print(f\"â“ è³ªå•: {question}\")\n",
        "print(f\"\\nğŸ’¡ å›ç­”: {result['result']}\")\n",
        "print(f\"\\nğŸ“š å‚ç…§ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ:\")\n",
        "for doc in result['source_documents']:\n",
        "    print(f\"  - {doc.page_content[:50]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvg1sZnYwrrF"
      },
      "source": [
        "---\n",
        "\n",
        "## ğŸ¨ Step 4: ã‚«ã‚¹ã‚¿ãƒ ãƒ„ãƒ¼ãƒ«ã¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ\n",
        "\n",
        "ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ã€ä¸ãˆã‚‰ã‚ŒãŸã‚¿ã‚¹ã‚¯ã‚’é”æˆã™ã‚‹ãŸã‚ã«ã€åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ã‚’è‡ªå¾‹çš„ã«é¸æŠã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuFPtuxRwrrF",
        "outputId": "732aba4e-222b-4cba-9e44-0c8c2d165cf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ¤” è³ªå•: 15 * 23ã‚’è¨ˆç®—ã—ã¦ãã ã•ã„\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mæ•°å¼ã‚’è¨ˆç®—ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™\n",
            "Action: calculate\n",
            "Action Input: '15 * 23'\u001b[0m\u001b[36;1m\u001b[1;3mè¨ˆç®—çµæœ: 15 * 23\u001b[0m\u001b[32;1m\u001b[1;3mè¨ˆç®—çµæœãŒå‡ºåŠ›ã•ã‚Œã¾ã—ãŸ\n",
            "Final Answer: 345\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "âœ… çµæœ: 345\n",
            "\n",
            "ğŸ¤” è³ªå•: 'Hello world this is a test'ã®å˜èªæ•°ã‚’æ•™ãˆã¦ãã ã•ã„\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mãƒ†ã‚­ã‚¹ãƒˆã®å˜èªæ•°ã‚’æ•°ãˆã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
            "Action: Action Input: get_word_count('Hello world this is a test')\u001b[0m is not a valid tool, try one of [calculate, get_word_count].\u001b[32;1m\u001b[1;3mI made a mistake, I should use get_word_count() instead of calculate().\n",
            "Action: Action Input: get_word_count('Hello world this is a test')\u001b[0m is not a valid tool, try one of [calculate, get_word_count].\u001b[32;1m\u001b[1;3mI need to provide the correct input format for get_word_count().\n",
            "Action: Action Input: get_word_count('Hello world this is a test')\u001b[0m is not a valid tool, try one of [calculate, get_word_count].\u001b[32;1m\u001b[1;3mI made a mistake in the format of the input. I should remove the single quotes around the text.\n",
            "Action: Action Input: get_word_count(Hello world this is a test)\u001b[0m is not a valid tool, try one of [calculate, get_word_count].\u001b[32;1m\u001b[1;3mI need to remember to use quotes around the text input for get_word_count().\n",
            "Action: Action Input: get_word_count(\"Hello world this is a test\")\u001b[0m is not a valid tool, try one of [calculate, get_word_count].\u001b[32;1m\u001b[1;3mI need to remember that the tool get_word_count() doesn't return a string, but rather an integer.\n",
            "Action: Action Input: get_word_count(\"Hello world this is a test\")\u001b[0m is not a valid tool, try one of [calculate, get_word_count].\u001b[32;1m\u001b[1;3mI made a mistake in how I was calling the get_word_count() function. I need to use the correct syntax.\n",
            "Action: Action Input: get_word_count(\"Hello world this is a test\")\u001b[0m is not a valid tool, try one of [calculate, get_word_count].\u001b[32;1m\u001b[1;3mI have been mistakenly trying to use the get_word_count() tool, which is not available. I should use the calculate() tool instead.\n",
            "Action: calculate(\"Hello world this is a test\")\u001b[0mInvalid Format: Missing 'Action Input:' after 'Action:'\u001b[32;1m\u001b[1;3mI made a mistake in my formatting. I need to provide the correct input format for the calculate() tool.\n",
            "Action: Action Input: calculate(\"Hello world this is a test\")\u001b[0m is not a valid tool, try one of [calculate, get_word_count].\u001b[32;1m\u001b[1;3mI have been mistakenly trying to use the get_word_count() tool, which is not available. I should use the calculate() tool instead.\n",
            "Action: calculate(\"Hello world this is a test\")\u001b[0mInvalid Format: Missing 'Action Input:' after 'Action:'\u001b[32;1m\u001b[1;3mI need to provide the correct input format for the calculate() tool.\n",
            "Action: Action Input: calculate(\"Hello world this is a test\")\u001b[0m is not a valid tool, try one of [calculate, get_word_count].\u001b[32;1m\u001b[1;3mI have been trying to use the wrong tool. I need to use the get_word_count() function to count the words in the text.\n",
            "Action: get_word_count(\"Hello world this is a test\")\u001b[0mInvalid Format: Missing 'Action Input:' after 'Action:'\u001b[32;1m\u001b[1;3mI have been consistently making a mistake in my formatting. I should provide the correct input format for the get_word_count() tool.\n",
            "Action: Action Input: get_word_count(\"Hello world this is a test\")\u001b[0m is not a valid tool, try one of [calculate, get_word_count].\u001b[32;1m\u001b[1;3mI have tried using both tools incorrectly. I should correctly use the get_word_count() tool to count the words in the text.\n",
            "Action: get_word_count(\"Hello world this is a test\")\u001b[0mInvalid Format: Missing 'Action Input:' after 'Action:'\u001b[32;1m\u001b[1;3mI am unable to use the tools provided to count the words in the text \"Hello world this is a test\". I should manually count the words in the text.\n",
            "Final Answer: 6\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "âœ… çµæœ: 6\n"
          ]
        }
      ],
      "source": [
        "from langchain.tools import tool\n",
        "from langchain.agents import create_react_agent, AgentExecutor\n",
        "from langchain import hub\n",
        "\n",
        "# ã‚«ã‚¹ã‚¿ãƒ ãƒ„ãƒ¼ãƒ«ã®ä½œæˆ\n",
        "@tool\n",
        "def calculate(expression: str) -> str:\n",
        "    \"\"\"æ•°å¼ã‚’è¨ˆç®—ã—ã¾ã™ã€‚ä¾‹: '2 + 2', '10 * 5'\"\"\"\n",
        "    try:\n",
        "        result = eval(expression)\n",
        "        return f\"è¨ˆç®—çµæœ: {result}\"\n",
        "    except:\n",
        "        return \"è¨ˆç®—ã‚¨ãƒ©ãƒ¼: æœ‰åŠ¹ãªæ•°å¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„\"\n",
        "\n",
        "@tool\n",
        "def get_word_count(text: str) -> str:\n",
        "    \"\"\"ãƒ†ã‚­ã‚¹ãƒˆã®å˜èªæ•°ã‚’æ•°ãˆã¾ã™\"\"\"\n",
        "    words = text.split()\n",
        "    return f\"å˜èªæ•°: {len(words)}èª\"\n",
        "\n",
        "# ãƒ„ãƒ¼ãƒ«ã®ãƒªã‚¹ãƒˆ\n",
        "tools = [calculate, get_word_count]\n",
        "\n",
        "# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®å–å¾—ï¼ˆReActãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰\n",
        "prompt = hub.pull(\"hwchase17/react\")\n",
        "\n",
        "# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ä½œæˆ\n",
        "agent = create_react_agent(\n",
        "    llm=llm,\n",
        "    tools=tools,\n",
        "    prompt=prompt\n",
        ")\n",
        "\n",
        "# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¨ã‚°ã‚¼ã‚­ãƒ¥ãƒ¼ã‚¿ãƒ¼ã®ä½œæˆ\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent,\n",
        "    tools=tools,\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True\n",
        ")\n",
        "\n",
        "# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè¡Œ\n",
        "questions = [\n",
        "    \"15 * 23ã‚’è¨ˆç®—ã—ã¦ãã ã•ã„\",\n",
        "    \"'Hello world this is a test'ã®å˜èªæ•°ã‚’æ•™ãˆã¦ãã ã•ã„\"\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    print(f\"\\nğŸ¤” è³ªå•: {q}\")\n",
        "    result = agent_executor.invoke({\"input\": q})\n",
        "    print(f\"âœ… çµæœ: {result['output']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaza3rbQwrrF"
      },
      "source": [
        "---\n",
        "\n",
        "## ğŸ“Š Step 5: å‡ºåŠ›ãƒ‘ãƒ¼ã‚µãƒ¼\n",
        "\n",
        "æ§‹é€ åŒ–ã•ã‚ŒãŸå‡ºåŠ›ã‚’å¾—ã‚‹ãŸã‚ã«ã€å‡ºåŠ›ãƒ‘ãƒ¼ã‚µãƒ¼ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1x29wNQwrrF",
        "outputId": "ff223243-c448-4a7e-ae40-e7ca34409cac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1548686155.py:31: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  output = chain.run({})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“š ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°åˆå¿ƒè€…å‘ã‘ãŠã™ã™ã‚æœ¬\n",
            "\n",
            "1. ã€Pythonè¨€èªãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°å…¥é–€ã€\n",
            "   è‘—è€…: å®®æœ¬å’Œå…¸\n",
            "   é›£æ˜“åº¦: â­â­\n",
            "   ç†ç”±: Pythonã®åŸºæœ¬ã‹ã‚‰å¿œç”¨ã¾ã§ä¸å¯§ã«è§£èª¬ã—ã¦ãŠã‚Šã€åˆå¿ƒè€…ã«ã‚‚ã‚ã‹ã‚Šã‚„ã™ã„å†…å®¹ã¨ãªã£ã¦ã„ã¾ã™ã€‚\n",
            "\n",
            "2. ã€Head First Java ç¬¬2ç‰ˆã€\n",
            "   è‘—è€…: Kathy Sierra, Bert Bates\n",
            "   é›£æ˜“åº¦: â­â­â­\n",
            "   ç†ç”±: Javaã®å­¦ç¿’ã«ã¯æ¬ ã‹ã›ãªã„ä¸€å†Šã§ã‚ã‚Šã€ãƒ¦ãƒ¼ãƒ¢ã‚¢æº¢ã‚Œã‚‹è§£èª¬ãŒç†è§£ã‚’åŠ©ã‘ã¦ãã‚Œã¾ã™ã€‚\n",
            "\n",
            "3. ã€JavaScriptæœ¬æ ¼å…¥é–€ã€\n",
            "   è‘—è€…: å±±ç”°ç¥¥å¯›\n",
            "   é›£æ˜“åº¦: â­â­â­\n",
            "   ç†ç”±: JavaScriptã®åŸºæœ¬ã‹ã‚‰å¿œç”¨ã¾ã§ã‚’ç¶²ç¾…ã—ãŸå†…å®¹ã§ã‚ã‚Šã€å®Ÿè·µçš„ãªå­¦ç¿’ãŒã§ãã¾ã™ã€‚\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "# å‡ºåŠ›å½¢å¼ã®å®šç¾©\n",
        "class BookRecommendation(BaseModel):\n",
        "    title: str = Field(description=\"æœ¬ã®ã‚¿ã‚¤ãƒˆãƒ«\")\n",
        "    author: str = Field(description=\"è‘—è€…å\")\n",
        "    reason: str = Field(description=\"æ¨è–¦ç†ç”±\")\n",
        "    difficulty: int = Field(description=\"é›£æ˜“åº¦ï¼ˆ1-5ï¼‰\")\n",
        "\n",
        "class BookList(BaseModel):\n",
        "    books: List[BookRecommendation] = Field(description=\"æ¨è–¦æ›¸ç±ã®ãƒªã‚¹ãƒˆ\")\n",
        "\n",
        "# ãƒ‘ãƒ¼ã‚µãƒ¼ã®ä½œæˆ\n",
        "parser = PydanticOutputParser(pydantic_object=BookList)\n",
        "\n",
        "# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ\n",
        "template = PromptTemplate(\n",
        "    template=\"\"\"ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°åˆå¿ƒè€…å‘ã‘ã®æœ¬ã‚’3å†Šæ¨è–¦ã—ã¦ãã ã•ã„ã€‚\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚\"\"\",\n",
        "    input_variables=[],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "# ãƒã‚§ãƒ¼ãƒ³ã®ä½œæˆã¨å®Ÿè¡Œ\n",
        "chain = LLMChain(llm=llm, prompt=template)\n",
        "output = chain.run({})\n",
        "\n",
        "# ãƒ‘ãƒ¼ã‚¹\n",
        "try:\n",
        "    result = parser.parse(output)\n",
        "    print(\"ğŸ“š ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°åˆå¿ƒè€…å‘ã‘ãŠã™ã™ã‚æœ¬\\n\")\n",
        "    for i, book in enumerate(result.books, 1):\n",
        "        print(f\"{i}. ã€{book.title}ã€\")\n",
        "        print(f\"   è‘—è€…: {book.author}\")\n",
        "        print(f\"   é›£æ˜“åº¦: {'â­' * book.difficulty}\")\n",
        "        print(f\"   ç†ç”±: {book.reason}\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}\")\n",
        "    print(f\"ç”Ÿã®å‡ºåŠ›: {output}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EOjRfJNwrrF"
      },
      "source": [
        "---\n",
        "\n",
        "## ğŸƒ ç·´ç¿’å•é¡Œ\n",
        "\n",
        "ã“ã“ã¾ã§å­¦ã‚“ã å†…å®¹ã‚’æ´»ç”¨ã—ã¦ã€ä»¥ä¸‹ã®èª²é¡Œã«æŒ‘æˆ¦ã—ã¦ã¿ã¾ã—ã‚‡ã†ï¼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8dBw1AJwrrF"
      },
      "outputs": [],
      "source": [
        "# ç·´ç¿’å•é¡Œ1: æ–™ç†ãƒ¬ã‚·ãƒ”ç”Ÿæˆãƒã‚§ãƒ¼ãƒ³ã‚’ä½œæˆ\n",
        "# ãƒ’ãƒ³ãƒˆ: ææ–™ã‚’å…¥åŠ›ã¨ã—ã¦ã€ãƒ¬ã‚·ãƒ”ã‚’ç”Ÿæˆã™ã‚‹ãƒã‚§ãƒ¼ãƒ³ã‚’ä½œã£ã¦ã¿ã¾ã—ã‚‡ã†\n",
        "\n",
        "# ã‚ãªãŸã®ã‚³ãƒ¼ãƒ‰ã‚’ã“ã“ã«æ›¸ã„ã¦ãã ã•ã„\n",
        "recipe_template = PromptTemplate(\n",
        "    input_variables=[\"ingredients\"],\n",
        "    template=\"\"\"ä»¥ä¸‹ã®ææ–™ã‚’ä½¿ã£ã¦ä½œã‚Œã‚‹æ–™ç†ã®ãƒ¬ã‚·ãƒ”ã‚’1ã¤ææ¡ˆã—ã¦ãã ã•ã„ï¼š\n",
        "\n",
        "ææ–™: {ingredients}\n",
        "\n",
        "ãƒ¬ã‚·ãƒ”ã«ã¯ä»¥ä¸‹ã‚’å«ã‚ã¦ãã ã•ã„ï¼š\n",
        "- æ–™ç†å\n",
        "- å¿…è¦ãªèª¿ç†æ™‚é–“\n",
        "- æ‰‹é †ï¼ˆç®‡æ¡æ›¸ãï¼‰\n",
        "- ãƒ¯ãƒ³ãƒã‚¤ãƒ³ãƒˆã‚¢ãƒ‰ãƒã‚¤ã‚¹\"\"\"\n",
        ")\n",
        "\n",
        "# ãƒã‚§ãƒ¼ãƒ³ã‚’ä½œæˆã—ã¦å®Ÿè¡Œã—ã¦ã¿ã¾ã—ã‚‡ã†\n",
        "# recipe_chain = ...\n",
        "# result = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xOOqI-rwrrG"
      },
      "outputs": [],
      "source": [
        "# ç·´ç¿’å•é¡Œ2: æ„Ÿæƒ…åˆ†æãƒ„ãƒ¼ãƒ«ã®ä½œæˆ\n",
        "# ãƒ’ãƒ³ãƒˆ: ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã¨ã—ã¦ã€æ„Ÿæƒ…ï¼ˆãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–/ä¸­ç«‹ï¼‰ã‚’åˆ¤å®š\n",
        "\n",
        "# ã‚ãªãŸã®ã‚³ãƒ¼ãƒ‰ã‚’ã“ã“ã«æ›¸ã„ã¦ãã ã•ã„\n",
        "from enum import Enum\n",
        "\n",
        "class Sentiment(str, Enum):\n",
        "    positive = \"ãƒã‚¸ãƒ†ã‚£ãƒ–\"\n",
        "    negative = \"ãƒã‚¬ãƒ†ã‚£ãƒ–\"\n",
        "    neutral = \"ä¸­ç«‹\"\n",
        "\n",
        "class SentimentAnalysis(BaseModel):\n",
        "    sentiment: Sentiment = Field(description=\"æ„Ÿæƒ…ã®åˆ†é¡\")\n",
        "    confidence: float = Field(description=\"ç¢ºä¿¡åº¦ï¼ˆ0-1ï¼‰\")\n",
        "    explanation: str = Field(description=\"åˆ¤å®šç†ç”±\")\n",
        "\n",
        "# ãƒ‘ãƒ¼ã‚µãƒ¼ã¨ãƒã‚§ãƒ¼ãƒ³ã‚’ä½œæˆã—ã¦å®Ÿè£…ã—ã¦ã¿ã¾ã—ã‚‡ã†\n",
        "# sentiment_parser = ...\n",
        "# sentiment_chain = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-06Ahu-wrrG"
      },
      "source": [
        "---\n",
        "\n",
        "## ğŸ“š ã•ã‚‰ãªã‚‹å­¦ç¿’ãƒªã‚½ãƒ¼ã‚¹\n",
        "\n",
        "### å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ\n",
        "- [LangChain Documentation](https://python.langchain.com/docs/get_started/introduction)\n",
        "- [LangChain API Reference](https://api.python.langchain.com/)\n",
        "\n",
        "### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
        "1. **ã‚ˆã‚Šé«˜åº¦ãªRAGã®å®Ÿè£…**: ã‚ˆã‚Šå¤§ããªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚»ãƒƒãƒˆã§ã®æ¤œç´¢\n",
        "2. **ã‚«ã‚¹ã‚¿ãƒ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ**: ç‹¬è‡ªã®ãƒ„ãƒ¼ãƒ«ã‚»ãƒƒãƒˆã‚’æŒã¤ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ä½œæˆ\n",
        "3. **ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å®Ÿè£…\n",
        "4. **LangSmith**: ãƒ‡ãƒãƒƒã‚°ã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°\n",
        "5. **LangServe**: APIã¨ã—ã¦ã®ãƒ‡ãƒ—ãƒ­ã‚¤\n",
        "\n",
        "### ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£\n",
        "- [GitHub](https://github.com/langchain-ai/langchain)\n",
        "- [Discord](https://discord.gg/langchain)\n",
        "- [Twitter](https://twitter.com/langchainai)\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ‰ ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼\n",
        "\n",
        "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§LangChainã®åŸºç¤ã‚’å­¦ã³ã¾ã—ãŸã€‚ã“ã“ã§å­¦ã‚“ã æ¦‚å¿µã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€\n",
        "ã‚ˆã‚Šè¤‡é›‘ã§å®Ÿç”¨çš„ãªAIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰ã§ãã¾ã™ã€‚\n",
        "\n",
        "**Happy Coding! ğŸš€**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VMIVaFQwrrG"
      },
      "source": [
        "---\n",
        "\n",
        "## ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°\n",
        "\n",
        "### ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ–¹æ³•\n",
        "\n",
        "1. **APIã‚­ãƒ¼ã‚¨ãƒ©ãƒ¼**:\n",
        "   - OpenAIã®APIã‚­ãƒ¼ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\n",
        "   - APIã‚­ãƒ¼ã®æœ‰åŠ¹æœŸé™ã‚„åˆ©ç”¨åˆ¶é™ã‚’ç¢ºèª\n",
        "\n",
        "2. **ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼**:\n",
        "   - ã™ã¹ã¦ã®å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\n",
        "   - Google Colabã®ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’å†èµ·å‹•ã—ã¦ã¿ã‚‹\n",
        "\n",
        "3. **ãƒ¡ãƒ¢ãƒªä¸è¶³**:\n",
        "   - ã‚ˆã‚Šå°ã•ã„ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨\n",
        "   - ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ¸›ã‚‰ã™\n",
        "\n",
        "4. **ãƒ¬ãƒ¼ãƒˆåˆ¶é™**:\n",
        "   - APIå‘¼ã³å‡ºã—ã®é–“ã«é©åˆ‡ãªé…å»¶ã‚’å…¥ã‚Œã‚‹\n",
        "   - ã‚ˆã‚Šé«˜ã„ãƒ†ã‚£ã‚¢ã®APIãƒ—ãƒ©ãƒ³ã‚’æ¤œè¨"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}